# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I1YGA84y_BNYkDgFhpo-Re3l4JxKbX9K
"""
import os
import torchaudio
from moviepy.editor import VideoFileClip
import cv2
from transformers import AutoModelForImageClassification, AutoImageProcessor, pipeline, Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments
import librosa
import soundfile as sf
from PIL import Image
import torch
import numpy as np
from datasets import load_dataset, DatasetDict

# Force CPU for all operations
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = ""
torch.set_default_device("cpu")  # Force tensor creation on CPU
torch.backends.mps.is_available = lambda: False  # Disable MPS detection
device = torch.device("cpu")

# Utility to move model to device recursively
def move_to_device(model, device):
    model.to(device)
    for param in model.parameters():
        if param.device != device:
            print(f"Warning: Parameter on {param.device}, moving to {device}")
            param.data = param.data.to(device)
    for buffer in model.buffers():
        if buffer.device != device:
            print(f"Warning: Buffer on {buffer.device}, moving to {device}")
            buffer.data = buffer.data.to(device)
    return model

# Extract audio from video
def extract_audio(video_path, audio_output_path="audio.ogg"):
    try:
        video = VideoFileClip(video_path)
        video.audio.write_audiofile(audio_output_path)
        video.close()
        print(f"Audio extracted to {audio_output_path}")
        return audio_output_path
    except Exception as e:
        print(f"Error extracting audio: {e}")
        return None

# Extract frames from video
def extract_frames(video_path, output_dir="frames", frame_interval=1):
    try:
        os.makedirs(output_dir, exist_ok=True)
        vidcap = cv2.VideoCapture(video_path)
        frames = []
        count = 0
        while vidcap.isOpened():
            success, image = vidcap.read()
            if not success:
                break
            if count % frame_interval == 0:  # Extract every 10th frame
                frame_path = os.path.join(output_dir, f"frame_{count}.jpg")
                cv2.imwrite(frame_path, image)
                frames.append(frame_path)
            count += 1
        vidcap.release()
        print(f"Extracted {len(frames)} frames to {output_dir}")
        return frames
    except Exception as e:
        print(f"Error extracting frames: {e}")
        return []


# Analyze facial emotions in frames
def analyze_facial_emotions(frame_paths):
    try:
        model_name = "trpakov/vit-face-expression"
        model = AutoModelForImageClassification.from_pretrained(model_name).to(device)
        processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)
        model.eval()
        results = []
        for frame_path in frame_paths:
            if not os.path.exists(frame_path):
                print(f"Error: Frame {frame_path} does not exist")
                continue
            frame = cv2.imread(frame_path)
            if frame is None:
                print(f"Warning: Could not load frame {frame_path}")
                continue
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(frame_rgb)
            inputs = processor(image, return_tensors="pt")["pixel_values"].to(device)
            with torch.no_grad():
                logits = model(inputs).logits
                probs = torch.softmax(logits, dim=-1)
                top_idx = torch.argmax(probs, dim=-1).item()
                top_prob = probs[0, top_idx].item()
                top_emotion = model.config.id2label[top_idx]
            if top_prob < 0.5:
                print(f"Warning: Low confidence ({top_prob}) for {frame_path}, skipping")
                continue
            results.append({
                "frame": frame_path,
                "emotion": top_emotion,
                "score": float(top_prob)
            })
        return results
    except Exception as e:
        print(f"Error in facial emotion analysis: {e}")
        return []
        
# Analyze speech emotions
def preprocess_audio_for_emotion(audio_path, target_sr=16000):
    try:
        if not os.path.exists(audio_path):
            print(f"Error: Audio file {audio_path} does not exist")
            return None
        speech_array, sampling_rate = torchaudio.load(audio_path)
        if sampling_rate != target_sr:
            resampler = torchaudio.transforms.Resample(sampling_rate, target_sr)
            speech_array = resampler(speech_array)
        if speech_array.dim() > 1 and speech_array.shape[0] > 1:
            speech_array = speech_array[0]
        speech_array = speech_array.squeeze()
        audio_array = speech_array.numpy()
        temp_path = "temp_audio.wav"
        sf.write(temp_path, audio_array, target_sr)
        return temp_path
    except Exception as e:
        print(f"Error in audio preprocessing for {audio_path}: {e}")
        return None

def analyze_speech_emotions(audio_path):
    try:
        processed_path = preprocess_audio_for_emotion(audio_path)
        if processed_path is None:
            return []
        
        # Use the pipeline for audio classification
        audio_classifier = pipeline("audio-classification", model="superb/wav2vec2-base-superb-er", device=device)
        result = audio_classifier(processed_path)
        
        # Debug: Print the result
        print(f"Speech Emotion Analysis Result: {result}")
        
        # Clean up temporary file
        if os.path.exists(processed_path):
            os.remove(processed_path)
        
        # Return the full list of emotions
        return [{"label": emotion["label"], "score": round(emotion["score"], 2)} for emotion in result]
    except Exception as e:
        print(f"Error in speech emotion analysis: {e}")
        return []

# Transcribe audio
def transcribe_audio(audio_path):
    try:
        transcriber = pipeline("automatic-speech-recognition", model="openai/whisper-base", device=device)
        transcription = transcriber(audio_path)
        return transcription["text"]
    except Exception as e:
        print(f"Error in transcription: {e}")
        return ""
    

# Analyze sentiment
def analyze_sentiment(text):
    try:
        sentiment_analyzer = pipeline("sentiment-analysis", model="siebert/sentiment-roberta-large-english", device=device)
        result = sentiment_analyzer(text)
        return {
            "label": result[0]["label"],
            "score": result[0]["score"]
        }
    except Exception as e:
        print(f"Error in sentiment analysis: {e}")
        return {}


# Summarize text
def summarize_text(text):
    try:
        summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=device)
        summary = summarizer(text, max_length=20, min_length=2, do_sample=False)
        return summary[0]["summary_text"]
    except Exception as e:
        print(f"Error in summarization: {e}")
        return ""



if __name__ == '__main__':
    # Example usage (only runs when model.py is executed directly)
    frame_paths = ["frames/frame_0.jpg", "frames/frame_5.jpg", "frames/frame_10.jpg", "frames/frame_15.jpg", "frames/frame_20.jpg"]
    facial_results = analyze_facial_emotions(frame_paths)
    for result in facial_results:
        print(f"Frame: {result['frame']}, Emotion: {result['emotion']}, Score: {result['score']}")

    # Example usage
    audio_path = "audio.ogg"
    speech_emotion = analyze_speech_emotions(audio_path)
    if speech_emotion:
        print(f"Speech Emotion: {speech_emotion['emotion']}, Score: {speech_emotion['score']}")
    else:
        print("No valid emotion detected")
        
    # Example usage
    transcription = transcribe_audio(audio_path)
    print(f"Transcription: {transcription}")
    
    # Example usage
    sentiment = analyze_sentiment(transcription)
    print(f"Sentiment: {sentiment['label']}, Score: {sentiment['score']}")
    
    # Example usage
    video_path = "video_angry.mkv"
    audio_path = extract_audio(video_path)
    
    # Example usage
    frame_paths = extract_frames(video_path, frame_interval=5)
        